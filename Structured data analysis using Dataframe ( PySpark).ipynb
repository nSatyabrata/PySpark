{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9cba99",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 12:18:21 WARN Utils: Your hostname, pc resolves to a loopback address: 127.0.1.1; using 192.168.170.52 instead (on interface wlp3s0)\n",
      "22/11/06 12:18:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 12:18:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.170.52:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3cdc1900a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb836b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(\"./data/kddcup.data.gz\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fcc775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+---------+---------+------+----------------+----------+-----------+-------+\n",
      "|Protocol|Service|flag|src_bytes|dst_bytes|urgent|num_failed_login|root_shell|guest_login|  label|\n",
      "+--------+-------+----+---------+---------+------+----------------+----------+-----------+-------+\n",
      "|     tcp|   http|  SF|      215|    45076|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      162|     4528|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      236|     1228|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      233|     2032|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      239|      486|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      238|     1282|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      235|     1337|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      234|     1364|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      239|     1295|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      181|     5450|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      184|      124|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      185|     9020|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      239|     1295|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      181|     5450|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      236|     1228|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      233|     2032|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      238|     1282|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      235|     1337|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      234|     1364|     0|               0|         0|          0|normal.|\n",
      "|     tcp|   http|  SF|      239|      486|     0|               0|         0|          0|normal.|\n",
      "+--------+-------+----+---------+---------+------+----------------+----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data \n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(df['value'], ',')\n",
    "\n",
    "df = df.withColumn('Protocol', split_col.getItem(1)) \\\n",
    "        .withColumn('Service', split_col.getItem(2)) \\\n",
    "        .withColumn('flag', split_col.getItem(3)) \\\n",
    "        .withColumn('src_bytes', split_col.getItem(4)) \\\n",
    "        .withColumn('dst_bytes', split_col.getItem(5)) \\\n",
    "        .withColumn('urgent', split_col.getItem(8)) \\\n",
    "        .withColumn('num_failed_login', split_col.getItem(10)) \\\n",
    "        .withColumn('root_shell', split_col.getItem(13)) \\\n",
    "        .withColumn('guest_login', split_col.getItem(21)) \\\n",
    "        .withColumn('label', split_col.getItem(41)) \\\n",
    "        .drop('value')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ba95be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before repartitions :  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After repartitions :  10\n"
     ]
    }
   ],
   "source": [
    "# increase the number of partitions\n",
    "print(\"Before repartitions : \",df.rdd.getNumPartitions())\n",
    "df = df.repartition(10)\n",
    "\n",
    "print(\"After repartitions : \",df.rdd.getNumPartitions())\n",
    "\n",
    "df.createOrReplaceTempView(\"df_kdd_cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8314da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|           label|  count|\n",
      "+----------------+-------+\n",
      "|          smurf.|2807886|\n",
      "|        neptune.|1072017|\n",
      "|         normal.| 972781|\n",
      "|          satan.|  15892|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|           nmap.|   2316|\n",
      "|           back.|   2203|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|            pod.|    264|\n",
      "|   guess_passwd.|     53|\n",
      "|buffer_overflow.|     30|\n",
      "|           land.|     21|\n",
      "|    warezmaster.|     20|\n",
      "|           imap.|     12|\n",
      "|        rootkit.|     10|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|       multihop.|      7|\n",
      "+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# count the number of connections for each label\n",
    "\n",
    "df.groupBy(\"label\").count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e494bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|Protocol|    State|   freq|\n",
      "+--------+---------+-------+\n",
      "|     udp|no attack| 191348|\n",
      "|     udp|   attack|   2940|\n",
      "|     tcp|no attack| 764894|\n",
      "|     tcp|   attack|1101613|\n",
      "|    icmp|   attack|2820782|\n",
      "|    icmp|no attack|  12763|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the list of protocols that are normal and vulnerable to attacks,\n",
    "# where there is NOT guest login to the destination addresses\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT Protocol,\n",
    "                CASE label\n",
    "                    WHEN 'normal.' THEN 'no attack'\n",
    "                    ELSE 'attack'\n",
    "                END AS State,\n",
    "                COUNT(*) as freq\n",
    "                FROM df_kdd_cup\n",
    "                WHERE guest_login = '0'\n",
    "                GROUP BY Protocol, State\n",
    "                ORDER BY Protocol DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d76dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+--------------+--------------+-----------+--------------------+---------------+\n",
      "|Protocol|    State|   Freq|mean_src_bytes|mean_dst_bytes|sum_uregent|sum_num_faked_logins|sum_guest_login|\n",
      "+--------+---------+-------+--------------+--------------+-----------+--------------------+---------------+\n",
      "|     tcp|no attack| 768670|       1844.29|       4071.32|       35.0|                96.0|         3776.0|\n",
      "|     udp|   attack|   2940|          26.4|          0.82|        0.0|                 0.0|            0.0|\n",
      "|     tcp|   attack|1101928|       4465.81|       2005.96|        4.0|                61.0|          315.0|\n",
      "|    icmp|   attack|2820782|        931.68|           0.0|        0.0|                 0.0|            0.0|\n",
      "|    icmp|no attack|  12763|         90.68|           0.0|        0.0|                 0.0|            0.0|\n",
      "|     udp|no attack| 191348|         98.32|         89.41|        0.0|                 0.0|            0.0|\n",
      "+--------+---------+-------+--------------+--------------+-----------+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A descriptive stats based on Protocols and Labels\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT Protocol,\n",
    "                CASE label\n",
    "                    WHEN 'normal.' THEN 'no attack'\n",
    "                    ELSE 'attack'\n",
    "                END AS State,\n",
    "                COUNT(*) AS Freq,\n",
    "                round(AVG(src_bytes),2) as mean_src_bytes,\n",
    "                round(AVG(dst_bytes),2) as mean_dst_bytes,\n",
    "                SUM(urgent) as sum_uregent,\n",
    "                SUM(num_failed_login) as sum_num_faked_logins,\n",
    "                SUM(guest_login) as sum_guest_login\n",
    "                FROM df_kdd_cup\n",
    "                GROUP BY Protocol, State  \n",
    "            \"\"\"\n",
    "\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee47ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+-------+\n",
      "|   service|  protocol|new_label|   freq|\n",
      "+----------+----------+---------+-------+\n",
      "|  other   |   udp    |    probe|    261|\n",
      "|  other   |   udp    |      U2R|      3|\n",
      "| private  |   udp    |      DoS|    979|\n",
      "|  ecr_i   |   icmp   |    probe|     59|\n",
      "| private  |   udp    |    probe|   1688|\n",
      "|  eco_i   |   icmp   |    probe|  12570|\n",
      "|  ecr_i   |   icmp   |      DoS|2808145|\n",
      "|  tim_i   |   icmp   |      DoS|      5|\n",
      "| domain_u |   udp    |    probe|      9|\n",
      "|  urp_i   |   icmp   |    probe|      3|\n",
      "+----------+----------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the frequency of sevices for the original UDP and ICMP based attacks\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def attack_category(item):\n",
    "    if item.replace(\".\",\"\") in ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop']:\n",
    "        return \"DoS\"\n",
    "    elif item.replace(\".\",\"\") in ['buffer_overflow', 'loadmodule', 'perl', 'rootkit']:\n",
    "        return \"U2R\"\n",
    "    elif item.replace(\".\",\"\") in ['dtp_write', 'guess_password', 'multihop', 'phf', 'spy', 'warezclient', 'warezmaster']:\n",
    "        return \"R2L\"\n",
    "    else:\n",
    "        return \"probe\"\n",
    "\n",
    "def center_justify(item):\n",
    "    return item.center(10)\n",
    "\n",
    "spark.udf.register(\"original_attack\", attack_category, StringType())\n",
    "spark.udf.register(\"center_justify\", center_justify, StringType())\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT\n",
    "                center_justify(Service) as service,\n",
    "                center_justify(Protocol) as protocol,\n",
    "                original_attack(label) as new_label,\n",
    "                COUNT(*) as freq\n",
    "                FROM df_kdd_cup\n",
    "                WHERE (Protocol = 'udp' or Protocol = 'icmp') and label != 'normal.'\n",
    "                GROUP BY service, new_label, protocol\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36382c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
